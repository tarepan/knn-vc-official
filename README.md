<div align="center">

# kNN-VC <!-- omit in toc -->
[![ColabBadge]][notebook]
[![PaperBadge]][paper]  

</div>

Clone of official kNN-VC, simple kNN-based voice conversion.  

<!-- Auto-generated by "Markdown All in One" extension -->
- [Demo](#demo)
- [Usage](#usage)
  - [Install](#install)
  - [Train](#train)
  - [Inference](#inference)
- [Results](#results)
- [References](#references)

![kNN-VC method](./knn-vc.png)

## Demo
[official demo][demo page].  

## Usage
### Install

```bash
# Python >=3.10
pip install "torch>=2" "torchaudio>=2" numpy
```

No kNN-VC install is needed. `torch.hub` handle everythingðŸ˜‰

### Inference

```python
import torch, torchaudio

src_wav_path = '<path to arbitrary 16kHz waveform>.wav'
ref_wav_paths = ['<path to arbitrary 16kHz waveform from target speaker>.wav', '<path to 2nd utterance from target speaker>.wav', ...]

knn_vc = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True)

query_seq = knn_vc.get_features(src_wav_path)
matching_set = knn_vc.get_matching_set(ref_wav_paths)

out_wav = knn_vc.match(query_seq, matching_set, topk=4)
# out_wav is (T,) tensor converted 16kHz output wav using k=4 for kNN.
```

Options:

- `knn_vc.match`
  - `topk`: int - Top K
- `torch.hub.load`
  - `prematched`: bool - Whether to use prematched model or non-prematched model

Note: the target speaker from `ref_wav_paths` _can be anything_, but should be clean speech from the desired speaker.  
The longer the cumulative duration of all reference waveforms, the better the quality will be (but the slower it will take to run).  
The improvement in quality diminishes beyond 5 minutes of reference speech.  

#### Checkpoints

Under the releases tab of this repo we provide three checkpoints:

- The frozen WavLM encoder taken from the [original WavLM authors](https://github.com/microsoft/unilm/tree/master/wavlm), which we host here for convenience and torch hub integration.
- The HiFiGAN vocoder trained on layer 6 of WavLM features.
- The HiFiGAN vocoder trained on **prematched** layer 6 of WavLM features (the best model in the paper).

For the HiFiGAN models we provide both the generator inference checkpoint and full training checkpoint with optimizer states.

The performance on the LibriSpeech dev-clean set is summarized:

| checkpoint | WER (%) | CER (%) | EER (%) |
| ----------- | :-----------: | :----: | :--: |
| [kNN-VC with prematched HiFiGAN](https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt) | 6.29 | 2.34 | 35.73 | 
| [kNN-VC with regular HiFiGAN](https://github.com/bshall/knn-vc/releases/download/v0.1/g_02500000.pt) | 6.39 | 2.41 | 32.55 | 


### Train

Install `librosa`, `tensorboard`, `matplotlib`, `fastprogress` and `scipy`.

1. **Precompute WavLM features of the vocoder dataset**: we provide a utility for this for the LibriSpeech dataset in `prematch_dataset.py`:

    ```bash
    usage: prematch_dataset.py [-h] --librispeech_path LIBRISPEECH_PATH
                            [--seed SEED] --out_path OUT_PATH [--device DEVICE]
                            [--topk TOPK] [--matching_layer MATCHING_LAYER]
                            [--synthesis_layer SYNTHESIS_LAYER] [--prematch]
                            [--resume]
    ```

    where you can specify `--prematch` or not to determine whether to use prematching when generating features or not. For example, to generate the dataset used to train the prematched HiFiGAN from the paper:
    `python prematch_dataset.py --librispeech_path /path/to/librispeech/root --out_path /path/where/you/want/outputs/to/go --topk 4 --matching_layer 6 --synthesis_layer 6 --prematch`

2. **Train HiFiGAN**: we adapt the training script from the [original HiFiGAN repo](https://github.com/jik876/hifi-gan) to work for WavLM features in `hifigan/train.py`. To train a hifigan model on the features you produced above:

    ```bash
    python -m hifigan.train --audio_root_path /path/to/librispeech/root/ --feature_root_path /path/to/the/output/of/previous/step/ --input_training_file data_splits/wavlm-hifigan-train.csv --input_validation_file data_splits/wavlm-hifigan-valid.csv --checkpoint_path /path/where/you/want/to/save/checkpoint --fp16 False --config hifigan/config_v1_wavlm.json --stdout_interval 25 --training_epochs 1800 --fine_tuning
    ```

    That's it! Once it is run up till 2.5M updates (or it starts to sound worse) you can stop training and use the pretrained checkpoint.


## Results
### Sample <!-- omit in toc -->
[Demo](#demo)

### Performance <!-- omit in toc -->
- training
  - xx [iter/sec] @ NVIDIA A100 on paperspace gradient Notebook (ConvTF32+/AMP+)
  - take about xx days for whole training
- inference
  - z.z [sec/sample] @ xx


## Official info
- HuBERT-Base work well ([issue#10](https://github.com/bshall/knn-vc/issues/10))


## References
### Original paper <!-- omit in toc -->
[![PaperBadge]][paper]  
<!-- Generated with the tool -> https://arxiv2bibtex.org/?q=2305.18975&format=bibtex -->
```bibtex
@misc{2305.18975,
Author = {Matthew Baas and Benjamin van Niekerk and Herman Kamper},
Title = {Voice Conversion With Just Nearest Neighbors},
Year = {2023},
Eprint = {arXiv:2305.18975},
}
```

### Acknowlegements <!-- omit in toc -->
- [Official kNN-VC](https://github.com/bshall/knn-vc)
- [HiFiGAN](https://github.com/jik876/hifi-gan)
- [WavLM](https://github.com/microsoft/unilm/tree/master/wavlm)


[ColabBadge]:https://colab.research.google.com/assets/colab-badge.svg

[paper]:https://arxiv.org/abs/2305.18975
[PaperBadge]:https://img.shields.io/badge/paper-arxiv.2305.18975-B31B1B.svg
[notebook]:https://colab.research.google.com/github/tarepan/knn-vc-official/blob/main/knnvc.ipynb
[demo page]:https://bshall.github.io/knn-vc/
